{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o89gtWRUAMgF"
   },
   "source": [
    "# Welcome!\n",
    "This is a reference implementation of Plug-and-Blend (https://github.com/xxbidiao/plug-and-blend , which itself is based on https://arxiv.org/abs/2104.04039), using the LogitsProcessor framework new in Huggingface Transformers. Feel free to check them out if you are unclear of anything in this notebook.\n",
    "\n",
    "# Set things up\n",
    "Here we will download necessary model to set up the modifier network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVxDrUWqACKB",
    "outputId": "bdfdb850-ad43-4868-ad15-939553571ed9",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install scipy\n",
    "!pip install tqdm\n",
    "\n",
    "# Imports\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, LogitsProcessorList, GPTNeoForCausalLM\n",
    "gedi_path = \"gedi_topic/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download the topic (modifier) model.\n",
    "\n",
    "# only run it the first time\n",
    "\n",
    "# !wget https://storage.googleapis.com/sfr-gedi-data/gedi_topic.zip\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('gedi_topic.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUBDHOehAJfT"
   },
   "source": [
    "Now let's set the Logits Processor up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2BruN48wAI2f",
    "outputId": "cdfc0a35-6384-4689-ece8-e0e3685ece99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at gedi_topic/ were not used when initializing GPT2LMHeadModel: ['logit_scale']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Set CUDA device to cuda if gpu is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gedi_location = gedi_path\n",
    "\n",
    "class PlugAndBlendLogitsProcessor(transformers.LogitsProcessor):\n",
    "\n",
    "    gedi_model = GPT2LMHeadModel.from_pretrained(gedi_location).to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # default omega from original GeDi work, higher disc_weight means more aggressive topic steering.\n",
    "    # can be overridden when calling generate_one_sentence(), see that function.\n",
    "    # default value (1x) is 30.\n",
    "    omega = 30\n",
    "\n",
    "    def __init__(self, topic: str, weight: float):\n",
    "        super().__init__()\n",
    "        self.topic = topic\n",
    "        self.weight = weight\n",
    "        self.encoded_topic = PlugAndBlendLogitsProcessor.tokenizer.encode(topic)[0]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        #print(\"Applying topic: %s, weight: %s\" % (self.encoded_topic, self.weight))\n",
    "        # print(\"test %s\" % scores[:, 100])\n",
    "        # scores[:, 100] += 1\n",
    "        # print(\"after %s\" % scores[:, 100])\n",
    "        modifiers = self.get_gedi_modifiers(input_ids = input_ids)\n",
    "\n",
    "        # Make them appear on the same device\n",
    "        modifiers = modifiers.to(scores.device)\n",
    "\n",
    "        scores += modifiers * self.weight * PlugAndBlendLogitsProcessor.omega\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def get_gedi_modifiers(self, input_ids):\n",
    "\n",
    "        # Setting up some constants\n",
    "        code_0 = \"negative\"\n",
    "        code_1 = \"positive\"\n",
    "        nt_id = PlugAndBlendLogitsProcessor.tokenizer.encode(code_0)[0]\n",
    "        pt_id = PlugAndBlendLogitsProcessor.tokenizer.encode(code_1)[0]\n",
    "\n",
    "        # define class weights for cross entropy loss: give weight 0 to [50256], the padding (eot) token.\n",
    "        crossentropy_loss_weight = [1] * 50257\n",
    "        crossentropy_loss_weight[50256] = 0 # do not calculate loss on eos token\n",
    "        crossentropy_loss_weight = torch.tensor(crossentropy_loss_weight).float().to(device)\n",
    "\n",
    "        # Creating prefixes.\n",
    "        seq_pt = (torch.ones(input_ids.shape[0]) * pt_id).type_as(input_ids).view(-1, 1)\n",
    "        seq_nt = (torch.ones(input_ids.shape[0]) * nt_id).type_as(input_ids).view(-1, 1)\n",
    "        encoded_topic_torch = (torch.ones(input_ids.shape[0]) * self.encoded_topic).type_as(input_ids).view(-1, 1)\n",
    "\n",
    "        # Assemble input_ids.\n",
    "        seq_pt_new = torch.cat((seq_pt, encoded_topic_torch, input_ids), dim=1)[:, :]\n",
    "        seq_nt_new = torch.cat((seq_nt, encoded_topic_torch, input_ids), dim=1)[:, :]\n",
    "\n",
    "        def prepare_inputs_for_generation(input_ids, **kwargs):\n",
    "            return {\"input_ids\": input_ids.to(device)}\n",
    "\n",
    "        seq_batched = torch.cat([seq_pt_new,seq_nt_new], dim=0)\n",
    "\n",
    "        model_inputs = prepare_inputs_for_generation(input_ids=seq_batched)\n",
    "\n",
    "        gedi_outputs = PlugAndBlendLogitsProcessor.gedi_model(**model_inputs)\n",
    "\n",
    "        # Let's calculate modifier on the whole sentence:\n",
    "        # This is modifier on all tokens multiplied.\n",
    "        # Here, we calculate the baseline (sentence without generated token) modifier, for normalization.\n",
    "\n",
    "        shift_logits = gedi_outputs[\"logits\"][..., :-1, :].contiguous().to(device)\n",
    "        shift_labels = seq_batched[..., 1:].contiguous().to(device)\n",
    "\n",
    "        # By using Cross Entropy on previous tokens,\n",
    "        # This effectively picked probabilities of previous tokens in the sequence.\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\",\n",
    "                                             weight=crossentropy_loss_weight,\n",
    "                                             )\n",
    "\n",
    "        # Cross entropy loss originally gives -p(x), so...\n",
    "        logits_r = -1 * loss_fct(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "        logits_r = logits_r.view(seq_batched.shape[0], -1)\n",
    "\n",
    "        seq_len = logits_r.shape[1]\n",
    "\n",
    "        logits_r = torch.sum(logits_r, 1)\n",
    "\n",
    "        # Now, finally add the baseline into the actual final (generated token) logits.\n",
    "        gedi_logits = torch.log_softmax(gedi_outputs[\"logits\"][:, -1, :], -1)\n",
    "        gedi_logits += logits_r.unsqueeze(1)\n",
    "\n",
    "        # Normalize modifier logits by sequence length and reshape it for output\n",
    "        gedi_logits_split = torch.split(gedi_logits / seq_len,\n",
    "                                        input_ids.shape[0])\n",
    "\n",
    "        logits = torch.stack(gedi_logits_split, 2)\n",
    "\n",
    "        logp_related_softmax = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # Once normalized, we only care about the \"positive\" dimension (0).\n",
    "        final_modifier = logp_related_softmax[...,0]\n",
    "\n",
    "        return final_modifier\n",
    "\n",
    "# Tests\n",
    "\n",
    "def test_generation(prompt = None, topics = None, print_out = False):\n",
    "    if prompt is None:\n",
    "      prompt = \"Once upon a time,\"\n",
    "    \n",
    "    if topics is None:\n",
    "      # default topics\n",
    "      topics = {\"Science\":1,\"Nature\":1}\n",
    "\n",
    "    \n",
    "\n",
    "    #print(transformers.__version__)\n",
    "\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Set up the base language model.\n",
    "    # As this is plug-and-blend, you can change this to any model that uses the GPT2 tokenizer (i.e. has the same input_ids => actual sentence mapping).\n",
    "    # We are using GPT-2 here just as an example.\n",
    "    #model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "    model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "\n",
    "    # Default prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    #input_ids = torch.cat([input_ids,input_ids,input_ids],dim=0)\n",
    "\n",
    "    lp_raw_list = []\n",
    "    for item in topics:\n",
    "      lp_raw_list.append(PlugAndBlendLogitsProcessor(topic=item, weight=topics[item]))\n",
    "    #lp_raw_list = [PlugAndBlendLogitsProcessor(topic=\"Science\", weight=1), PlugAndBlendLogitsProcessor(topic=\"Nature\", weight=1)]\n",
    "\n",
    "    lp_list = LogitsProcessorList(lp_raw_list)\n",
    "\n",
    "    greedy_output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        logits_processor=lp_list,\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "\n",
    "    result = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "    if print_out:\n",
    "        print(\"Output:\\n\" + 100 * '-')\n",
    "        print(result)\n",
    "    return result\n",
    "    # greedy_output = model.generate(\n",
    "    #     input_ids,\n",
    "    #     max_length=50,\n",
    "    #     logits_processor=lp_list,\n",
    "    # )\n",
    "    # print(\"Output:\\n\" + 100 * '-')\n",
    "    # print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPuKp9iUDgJZ"
   },
   "source": [
    "# Generate things (Demo)\n",
    "\n",
    "This demo showcases generation using GPT-2 as base model. Refer to the content of this function to see how you can use a different model (as long as its tokenizer is `GPT2Tokenizer.from_pretrained(\"gpt2\")` . \n",
    "\n",
    "Change test_prompt for prompt; change topics dictionary for topics you want to include in the generated sentence. 1 (all weights added up) gives standard control strength, and in our experiments 2 to 4 gives stronger steering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_9n-y3UAwQH",
    "outputId": "b1a51c89-a49e-4af4-f69c-096a174d4bcb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here is a fun story.\\n\\nA few years ago, I was invited to speak at a conference in the United States. I was asked to speak on the topic of “The Future of the Internet.”\\n\\nI was asked'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_topics = {\"World\":2}\n",
    "test_prompt = \"Here is a fun story.\"\n",
    "\n",
    "test_generation(prompt=test_prompt, topics=test_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  818,   257, 14702,  4917,    11,  5519,  5071,   257, 27638,   286,\n",
      "         28000, 19942,  2877,   287,   257,  6569]])\n",
      "tensor([[  818,   257, 14702,  4917,    11,  5519,  5071,   257, 27638,   286,\n",
      "         28000, 19942,  2877,   287,   257,  6569]])\n"
     ]
    }
   ],
   "source": [
    "# Test whether two tokenizers decode things the same way.\n",
    "tokenizer1 = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def get_inp(tok,pr):\n",
    "    return tok(pr, return_tensors=\"pt\").input_ids\n",
    "\n",
    "prompt = \"In a shocking finding, scientists discovered a herd of unicorns living in a remote\"\n",
    "\n",
    "print(get_inp(tokenizer,prompt))\n",
    "print(get_inp(tokenizer1,prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import scipy\n",
    "\n",
    "classification_pipeline = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "def classifier_scoring_full(topic1, topic2, text):\n",
    "    \"\"\"\n",
    "    Using a classifier, score text based on how close it is to topic 1.\n",
    "    :param topic1: topic1. the closer the better.\n",
    "    :param topic2: topic2. the further away the better.\n",
    "    :param text: text under consideration.\n",
    "    :return: dict containing:\n",
    "        score : score from -1 to 1.\n",
    "        entropy: uncertainty of the classifier from 0 to 1.\n",
    "    \"\"\"\n",
    "    global classification_pipeline\n",
    "    if classification_pipeline is None:\n",
    "        classification_pipeline = pipeline(\"zero-shot-classification\")\n",
    "    if len(text) == 0:\n",
    "        print(\"Empty sentence! using score=0.5 entropy=1\")\n",
    "        return {\n",
    "            \"score\":0.5,\n",
    "            \"entropy\":1,\n",
    "        }\n",
    "    result = classification_pipeline(text, [topic1, topic2])\n",
    "    # print(result)\n",
    "    for idx in range(2):\n",
    "        if result['labels'][idx] == topic1:\n",
    "            return result['scores'][idx]\n",
    "            # return {\n",
    "            #     \"score\": result['scores'][idx],\n",
    "            #     \"entropy\": scipy.stats.entropy(result['scores'])\n",
    "            # }\n",
    "    raise RuntimeError(\"Topic is missing from inferring results. Classification model may not be working.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8506259322166443"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_scoring_full(\"Technology\",\"Sun\",\"Computer programming languages are tools for people to teach what a machine should do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ranking_svm_loss(scores, should_increase=True):\n",
    "    \"\"\"\n",
    "    Calculate Ranking SVM loss based on Kendall's Tau-a coefficient.\n",
    "    :param scores: all scores.\n",
    "    :param should_increase: True means `scores` should be increasing.\n",
    "    :return: ranking svm loss (how unordered the list is)\n",
    "    \"\"\"\n",
    "    order = 1 if should_increase else -1\n",
    "    max_possible = len(scores) * (len(scores) - 1) / 2.0\n",
    "    same_order_count = 0\n",
    "    same_value_count = 0\n",
    "    for index1, x1 in enumerate(scores):\n",
    "        for index2, x2 in enumerate(scores):\n",
    "            if index1 >= index2:\n",
    "                continue  # skip considered ones.\n",
    "            if (x2 - x1) * order > 0:\n",
    "                same_order_count += 1\n",
    "            elif x1 == x2:\n",
    "                same_value_count += 1\n",
    "    different_order_count = max_possible - same_order_count - same_value_count\n",
    "    # best_raw_score = min(bigger_count,smaller_count)\n",
    "\n",
    "    # Since we now force an order as input\n",
    "    raw_score = same_order_count - different_order_count\n",
    "    normalized_raw_score = raw_score / max_possible\n",
    "    score = normalized_raw_score\n",
    "    return score\n",
    "\n",
    "def experiment_two_topic_weights(prompt, topic1, topic2, scorer=lambda x: 0):\n",
    "    # if scorer is not None and type(scorer) is not type(lambda x: x):\n",
    "    #     raise AttributeError(\"scorer has to be a function if passed in.\")\n",
    "\n",
    "    step_now = 0.0\n",
    "    end = 1.0\n",
    "    step = 0.25\n",
    "    all_scores = []\n",
    "    while step_now <= end:\n",
    "        all_topics_for_eval = {topic1: step_now, topic2: end - step_now}\n",
    "        text = test_generation(prompt=prompt, topics=all_topics_for_eval)\n",
    "\n",
    "        score = scorer(text)\n",
    "        # print(\"[%.4f]%s:\\n%s\"%(score,all_topics_for_eval,text))\n",
    "        step_now += step\n",
    "        all_scores.append(score)\n",
    "    order_score = ranking_svm_loss(all_scores, should_increase=True)\n",
    "    return order_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "topics = [\"Business\",\"Science\",\"World\",\"Sports\"]\n",
    "\n",
    "#topic1 = \"Business\"\n",
    "#topic2 = \"World\"\n",
    "\n",
    "prompts = [\n",
    "    \"The cat stretched.\",\n",
    "    \"Jacob stood on his tiptoes.\",\n",
    "    \"The car turned the corner.\",\n",
    "    \"Kelly twirled in circles.\",\n",
    "    \"She opened the door.\",\n",
    "    \"Aaron made a picture.\",\n",
    "    \"I'm sorry.\",\n",
    "    \"I danced.\",\n",
    "    \"Run!\",\n",
    "    \"Open the jar carefully.\",\n",
    "    \"Read the directions.\",\n",
    "    \"Don't cry.\",\n",
    "    \"Use common sense.\",\n",
    "    \"Make the best of things.\",\n",
    "    \"Catch up!\",\n",
    "    \"Sarah and Ira drove to the store.\",\n",
    "    \"Jenny and I opened all the gifts.\",\n",
    "    \"The cat and dog ate.\",\n",
    "    \"My parents and I went to a movie.\",\n",
    "    \"Mrs. Juarez and Mr. Smith are dancing gracefully.\",\n",
    "    \"Samantha, Elizabeth, and Joan are on the committee.\",\n",
    "    \"The ham, green beans, mashed potatoes, and corn are gluten-free.\",\n",
    "    \"The paper and pencil sat idle on the desk.\",\n",
    "]\n",
    "\n",
    "import logging\n",
    "import re\n",
    "def set_global_logging_level(level=logging.ERROR, prefices=[\"\"]):\n",
    "    \"\"\"\n",
    "    Override logging levels of different modules based on their name as a prefix.\n",
    "    It needs to be invoked after the modules have been loaded so that their loggers have been initialized.\n",
    "\n",
    "    Args:\n",
    "        - level: desired level. e.g. logging.INFO. Optional. Default is logging.ERROR\n",
    "        - prefices: list of one or more str prefices to match (e.g. [\"transformers\", \"torch\"]). Optional.\n",
    "          Default is `[\"\"]` to match all active loggers.\n",
    "          The match is a case-sensitive `module_name.startswith(prefix)`\n",
    "    \"\"\"\n",
    "    prefix_re = re.compile(fr'^(?:{ \"|\".join(prefices) })')\n",
    "    for name in logging.root.manager.loggerDict:\n",
    "        if re.match(prefix_re, name):\n",
    "            logging.getLogger(name).setLevel(level)\n",
    "\n",
    "set_global_logging_level()\n",
    "\n",
    "#prompts = [\"Test\"]\n",
    "\n",
    "all_result = {}\n",
    "\n",
    "for topic1 in topics:\n",
    "    for topic2 in topics:\n",
    "        if topic1 >= topic2:\n",
    "            continue\n",
    "        all_result_key = \"%s-%s\"%(topic1,topic2)\n",
    "        scoring_function = partial(classifier_scoring_full, topic1, topic2)\n",
    "        all_result[all_result_key] = []\n",
    "        for item in prompts:\n",
    "            score = experiment_two_topic_weights(item,topic1,topic2,scoring_function)\n",
    "            all_result[all_result_key].append(score)\n",
    "            print(\"%s:%s\"%(all_result,sum(all_result[all_result_key])/len(all_result[all_result_key])))\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "with open('result.txt','w') as f:\n",
    "    json.dump(all_result,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNUa/tiNgFsJ6MJQp503oka",
   "name": "blending-generation-logits-processor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}